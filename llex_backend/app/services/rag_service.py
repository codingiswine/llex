import asyncio
import hashlib
import json
import logging
import sqlite3
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, AsyncGenerator
from openai import AsyncOpenAI
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
from app.config import settings

logger = logging.getLogger("RAGService")

# SQLite ìºì‹œ ì„¤ì • (Docker í™˜ê²½ ê³ ë ¤)
CACHE_DIR = Path("/app/.cache") if Path("/app").exists() else Path(".cache")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
EMBEDDING_CACHE_DB = CACHE_DIR / "embedding_cache.db"

# âœ… ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš© (settingsì—ì„œ ê°€ì ¸ì˜´)
openai_client = settings.openai_client
qdrant_client = settings.qdrant_client  # AsyncQdrantClient

# SQLite ìºì‹œ ì´ˆê¸°í™”
def init_embedding_cache():
    conn = sqlite3.connect(EMBEDDING_CACHE_DB)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS embeddings (
            query_hash TEXT PRIMARY KEY,
            query_text TEXT,
            embedding TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

init_embedding_cache()

def get_embedding_cached(query: str) -> Optional[List[float]]:
    query_hash = hashlib.sha256(query.encode('utf-8')).hexdigest()
    conn = sqlite3.connect(EMBEDDING_CACHE_DB)
    cursor = conn.cursor()
    cursor.execute("SELECT embedding FROM embeddings WHERE query_hash = ?", (query_hash,))
    result = cursor.fetchone()
    conn.close()
    if result:
        logger.info(f"âœ… ì„ë² ë”© ìºì‹œ íˆíŠ¸: {query[:30]}...")
        return json.loads(result[0])
    return None

def save_embedding_cached(query: str, embedding: List[float]):
    query_hash = hashlib.sha256(query.encode('utf-8')).hexdigest()
    conn = sqlite3.connect(EMBEDDING_CACHE_DB)
    cursor = conn.cursor()
    cursor.execute(
        "INSERT OR REPLACE INTO embeddings (query_hash, query_text, embedding) VALUES (?, ?, ?)",
        (query_hash, query, json.dumps(embedding))
    )
    conn.commit()
    conn.close()
    logger.info(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥: {query[:30]}...")

async def get_embedding_async(query: str) -> List[float]:
    start_time = time.time()
    cached_embedding = get_embedding_cached(query)
    if cached_embedding:
        logger.info(f"â±ï¸ ì„ë² ë”© ì¡°íšŒ ì‹œê°„: {time.time() - start_time:.2f}s (ìºì‹œ)")
        return cached_embedding

    response = await openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=query
    )
    embedding = response.data[0].embedding
    save_embedding_cached(query, embedding)
    logger.info(f"â±ï¸ ì„ë² ë”© ìƒì„± ì‹œê°„: {time.time() - start_time:.2f}s (ì‹ ê·œ)")
    return embedding

async def search_qdrant_async(vector: List[float], limit: int = 5) -> List[Dict[str, Any]]:
    start_time = time.time()
    # âœ… AsyncQdrantClient ì‚¬ìš© (ë” ì´ìƒ to_thread ë¶ˆí•„ìš”)
    results = await qdrant_client.search(
        collection_name=settings.QDRANT_COLLECTION_NAME,
        query_vector=vector,
        limit=limit,
        with_payload=True
    )
    logger.info(f"â±ï¸ Qdrant ê²€ìƒ‰ ì‹œê°„: {time.time() - start_time:.2f}s")
    return [
        {
            "id": hit.id,
            "score": hit.score,
            "payload": hit.payload
        } for hit in results
    ]

def build_context(qdrant_results: List[Dict[str, Any]], max_chunk_length: int = 150) -> str:
    context_chunks = []
    for result in qdrant_results:
        payload = result.get("payload", {})
        law_name = payload.get("ë²•ë ¹ëª…", "ì•Œ ìˆ˜ ì—†ëŠ” ë²•ë ¹")
        article_num = payload.get("ì¡°ë¬¸ë²ˆí˜¸", "ì•Œ ìˆ˜ ì—†ëŠ” ì¡°ë¬¸")
        paragraph_num = payload.get("í•­ë²ˆí˜¸", "")
        sub_paragraph_num = payload.get("í˜¸ë²ˆí˜¸", "")
        content = payload.get("ë³¸ë¬¸", "")
        enforcement_date = payload.get("ì‹œí–‰ì¼ì", "ì•Œ ìˆ˜ ì—†ìŒ")

        # ì¡°ë¬¸ë²ˆí˜¸, í•­ë²ˆí˜¸, í˜¸ë²ˆí˜¸ë¥¼ ìƒì„¸í•˜ê²Œ í¬í•¨
        full_article_ref = f"ì œ{article_num}ì¡°"
        if paragraph_num:
            full_article_ref += f" ì œ{paragraph_num}í•­"
        if sub_paragraph_num:
            full_article_ref += f" ì œ{sub_paragraph_num}í˜¸"

        chunk = f"ë²•ë ¹ëª…: {law_name}, ì¡°í•­: {full_article_ref}, ì‹œí–‰ì¼ì: {enforcement_date}, ë‚´ìš©: {content}"
        
        # ì²­í¬ ê¸¸ì´ ì œí•œ
        if len(chunk) > max_chunk_length:
            chunk = chunk[:max_chunk_length] + "..."
        context_chunks.append(chunk)
    return "\n\n".join(context_chunks)

async def run_rag_async(query: str) -> AsyncGenerator[str, None]:
    start_total_time = time.time()

    # 1. ì„ë² ë”© ìƒì„± ë° Qdrant ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰
    embedding_vector = await get_embedding_async(query)
    qdrant_results = await search_qdrant_async(embedding_vector)

    # 2. ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
    context = build_context(qdrant_results)
    logger.info(f"ğŸ“š RAG Context:\n{context[:200]}...")

    # 3. GPT ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    from app.services.gpt_service import generate_answer_async  # âœ… ë‚´ë¶€ ì§€ì—° import
    async for token in generate_answer_async(query, context):
        yield token
    
    end_total_time = time.time()
    logger.info(f"â±ï¸ ì „ì²´ RAG ì‘ë‹µ ì‹œê°„: {end_total_time - start_total_time:.2f}s")
