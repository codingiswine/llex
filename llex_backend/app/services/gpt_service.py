#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
gpt_service.py (LLeX v5.2 - Async Stable)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ëª¨ë“  async í•¨ìˆ˜ì— await ì ìš©
- summarize_web ë¹„ë™ê¸° í˜¸ì¶œ ë³´ì™„
- DB / Memory ì™„ì „ ë¹„ë™ê¸° í˜¸í™˜
"""

import logging
import asyncio
import warnings
from typing import AsyncGenerator
from openai import AsyncOpenAI

# Suppress all LangChain warnings (including deprecation)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message=".*LangChain.*")

from langchain.memory import ConversationBufferMemory

try:
    # âœ… Docker ë‚´ë¶€ ê¸°ì¤€ (WORKDIR /app)
    from app.config import settings
    from app.services.rag_service import get_embedding_async, search_qdrant_async
    from app.tools.websearch_tool import summarize_web
    from app.tools.db_query_tool_async import get_recent_history
except ModuleNotFoundError:
    # âœ… ë¡œì»¬ ì‹¤í–‰ ê¸°ì¤€ (Cursor, VSCode)
    from app.config import settings
    from app.services.rag_service import get_embedding_async, search_qdrant_async
    from app.tools.websearch_tool import summarize_web
    from app.tools.db_query_tool_async import get_recent_history



logger = logging.getLogger("GPTService")
openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³ ì • ì‘ë‹µ ìºì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIXED_RESPONSES = {
    "ìš©ì‚°êµ¬ ì¬ë‚œì•ˆì „ê´€ë¦¬íŒ€ì´ ì•Œì•„ì•¼ í•  ë²•": """\
#### ğŸ›ï¸ ì¬ë‚œì•ˆì „ê´€ë¦¬íŒ€ í•µì‹¬ 9ê°œ ë²•ë ¹
1ï¸âƒ£ ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™  
2ï¸âƒ£ ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™  
3ï¸âƒ£ ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²• ì‹œí–‰ë ¹  
4ï¸âƒ£ ì‚°ì—…ì•ˆì „ë³´ê±´ë²•  
5ï¸âƒ£ ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²•  
6ï¸âƒ£ ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹  
7ï¸âƒ£ ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²• ì‹œí–‰ê·œì¹™  
8ï¸âƒ£ ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥   
9ï¸âƒ£ ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹  
> ğŸ’¬ ì´ ì¤‘ ê°€ì¥ ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ë²•ì€  
> **ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™**ì´ì•¼.
"""
}

# âœ… ì‚¬ìš©ìë³„ Memory ê´€ë¦¬
USER_MEMORIES = {}

def get_user_memory(user_id: str):
    """ì‚¬ìš©ìë³„ Memory ê°ì²´ ë°˜í™˜"""
    if user_id not in USER_MEMORIES:
        USER_MEMORIES[user_id] = ConversationBufferMemory(
            memory_key="chat_history",
            input_key="input",
            return_messages=False
        )
        print(f"ğŸ§  [init] {user_id} Memory ìƒì„± ì™„ë£Œ")
    return USER_MEMORIES[user_id]

def check_fixed_response(query: str) -> str | None:
    for key, value in FIXED_RESPONSES.items():
        if key in query or key.replace(" ", "") in query.replace(" ", ""):
            return value
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… Memory-aware GPT ë‹µë³€ ìƒì„±ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def generate_answer_async(user_id: str, full_prompt: str) -> AsyncGenerator[str, None]:
    fixed = check_fixed_response(full_prompt)
    if fixed:
        yield fixed
        return

    user_memory = get_user_memory(user_id)

    # ğŸ”§ await ì¶”ê°€: DB MemoryëŠ” ë¹„ë™ê¸° í•¨ìˆ˜
    history_records = await get_recent_history(user_id, limit=10)
    history_text = "\n".join(
        f"ì‚¬ìš©ì: {h['question']}\nLLeX.Ai: {h['answer']}"
        for h in history_records
    )

    past_context = user_memory.load_memory_variables({})
    chain_history = past_context.get("chat_history", "")

    merged_prompt = f"{history_text}\n{chain_history}\nì‚¬ìš©ì: {full_prompt}"

    messages = [
        {"role": "system", "content": "ë„ˆëŠ” LinkCampus ì¬ë‚œì•ˆì „ê´€ë¦¬íŒ€ì˜ ë²•ë ¹Â·ì•ˆì „ ì–´ì‹œìŠ¤í„´íŠ¸ LLeX.Aiì•¼."},
        {"role": "user", "content": merged_prompt},
    ]

    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.5,
        max_tokens=1000,
        stream=True,
    )

    full_answer = ""
    async for chunk in response:
        token = chunk.choices[0].delta.content
        if token:
            full_answer += token
            yield token

    user_memory.save_context({"input": full_prompt}, {"output": full_answer})
    print(f"ğŸ§  [Memory] {user_id} ëŒ€í™” ì €ì¥ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âš–ï¸ Hybrid RAG + Web í†µí•© (ë¹„ë™ê¸° ì™„ì „í™”)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def hybrid_merge(user_id: str, question: str):
    print("âš–ï¸ [hybrid_merge] ì‹¤í–‰ ì‹œì‘")

    fixed = check_fixed_response(question)
    if fixed:
        yield fixed
        return

    user_memory = get_user_memory(user_id)

    # ğŸ”§ await ì¶”ê°€
    history_records = await get_recent_history(user_id, limit=10)
    history_text = "\n".join(
        f"ì‚¬ìš©ì: {h['question']}\nLLeX.Ai: {h['answer']}"
        for h in history_records
    )

    past_context = user_memory.load_memory_variables({})
    chain_history = past_context.get("chat_history", "")

    # ğŸ”§ ë¹„ë™ê¸° ì‘ì—… ë³‘ë ¬ ì²˜ë¦¬
    rag_task = asyncio.create_task(_rag_search(question))
    web_task = asyncio.create_task(_web_search(question))
    rag_results, web_results = await asyncio.gather(rag_task, web_task)

    merged_prompt = f"""
{history_text}
{chain_history}

### ì‚¬ìš©ì ì§ˆë¬¸
{question}

### ë‚´ë¶€ ë²•ë ¹ ê·¼ê±° (RAG ê²°ê³¼)
{_format_rag_results(rag_results)}

### ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ê²°ê³¼
{_format_web_results(web_results)}

ğŸ’¡ ìœ„ì˜ ë‚´ìš©ì„ ì°¸ê³ í•´ ì •í™•í•˜ê³  ê·¼ê±° ìˆëŠ” ë‹µë³€ì„ ì‘ì„±í•´ì¤˜.
"""

    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” LinkCampusì˜ ì¬ë‚œì•ˆì „ê´€ë¦¬íŒ€ì„ ìœ„í•œ ë²•ë ¹Â·ì•ˆì „ ì–´ì‹œìŠ¤í„´íŠ¸ LLeX.Aiì•¼."},
            {"role": "user", "content": merged_prompt},
        ],
        temperature=0.3,
        stream=True,
    )

    full_answer = ""
    async for chunk in response:
        token = chunk.choices[0].delta.content
        if token:
            full_answer += token
            yield token

    print("âœ… [hybrid_merge] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

    try:
        user_memory.save_context({"input": question}, {"output": full_answer})
        print(f"ğŸ§  [Memory] hybrid_merge ëŒ€í™” ë‚´ìš© ì €ì¥ ì™„ë£Œ (user={user_id})")
    except Exception as e:
        print(f"âš ï¸ [Memory] ì €ì¥ ì‹¤íŒ¨: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ìœ í‹¸ í•¨ìˆ˜ë“¤ (ë¹„ë™ê¸° ìˆ˜ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _rag_search(question: str):
    try:
        embedding = await get_embedding_async(question)
        results = await search_qdrant_async(embedding, limit=3)
        print(f"ğŸ“š [RAG] ê²€ìƒ‰ ì™„ë£Œ ({len(results)}ê±´)")
        return results
    except Exception as e:
        print(f"âš ï¸ [RAG] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

async def _web_search(question: str):
    try:
        # ğŸ”§ summarize_webì€ ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ í˜¸ì¶œ
        result = await summarize_web(question)
        print(f"ğŸŒ [Web] ê²€ìƒ‰ ì™„ë£Œ")
        return result
    except Exception as e:
        print(f"âš ï¸ [Web] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {}

def _format_rag_results(results):
    if not results:
        return "ì—†ìŒ"
    return "\n".join(
        f"- **{r.get('payload', {}).get('title', 'ì œëª© ì—†ìŒ')}** "
        f"(score={r.get('score', 0):.2f})\n  {r.get('payload', {}).get('content', '')[:200]}"
        for r in results
    )

def _format_web_results(results):
    if not results or "summaries" not in results:
        return "ì—†ìŒ"
    return results.get("summaries", "").strip()

__all__ = ["generate_answer_async", "hybrid_merge", "check_fixed_response", "get_user_memory"]

print("âœ… [init] gpt_service.py ë¡œë“œ ì™„ë£Œ (Async Stable)")
